{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Felix Di Nezza IT DBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA SOURCES <br>\n",
    "\n",
    "Reddit Climate Change <br>\n",
    "Author: Lexyr <br>\n",
    "Source: https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset <br>\n",
    "Data collected using [SocialGrep Exports](https://socialgrep.com/exports) <br>\n",
    "\n",
    "Twitter data <br>\n",
    "Author: DEEPSENSE <br>\n",
    "source: provided during challenge <br>\n",
    "\n",
    "LICENCE <br>\n",
    "Attribution 4.0 International (CC BY 4.0) <br>\n",
    "https://creativecommons.org/licenses/by/4.0/ <br>\n",
    "\n",
    "CHANGES AND USAGE <br>\n",
    "Dataset used for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES\n",
    "\n",
    "Sentiment analysis on twitter <br>\n",
    "https://medium.com/mlearning-ai/elon-musks-twitter-sentiment-analysis-with-transformers-hugging-face-roberta-49b9e61b1433 <br>\n",
    "\n",
    "Tutorial nltk & roberta <br>\n",
    "https://www.youtube.com/watch?v=QpzMWQvxXWk <br>\n",
    "\n",
    "Python regex functions <br>\n",
    "https://pynative.com/python-regex-compile/ <br>\n",
    "\n",
    "Word cloud tutorial <br>\n",
    "https://medium.com/mcd-unison/create-word-cloud-scraping-data-from-reddit-api-using-praw-and-spacy-b5c9c61c2d10 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## additional installs\n",
    "#!pip install emoji\n",
    "#!pip install emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import pathlib\n",
    "import tqdm\n",
    "import string\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# nltk \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#emoji filter\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "with open ( './the-reddit-climate-change-dataset-comments.csv',\n",
    "            'r',\n",
    "            encoding=\"utf8\",\n",
    "            newline='\\n'\n",
    "          ) as source_csv: \n",
    "            reader = csv.reader(source_csv)\n",
    "            df = pd.read_csv(source_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print size of the dataframe\n",
    "print(df.shape)\n",
    "\n",
    "# return first row \n",
    "df['body'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dataframe information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first 5 rows to check content structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiparameter search to check for number of results returned\n",
    "# dataframe[column to visualize][search based on column]\n",
    "\n",
    "df[['body', 'subreddit.name']][\n",
    "           (\n",
    "           df['body'].str.contains('micro-plastic', na = False) |\n",
    "           df['body'].str.contains('microplastic', na = False) |\n",
    "           df['body'].str.contains('Microplastic', na = False) |\n",
    "           df['body'].str.contains('Micro-plastic', na = False)\n",
    "           ) &\n",
    "           (\n",
    "           df['body'].str.contains('ocean', na = False) |\n",
    "           df['body'].str.contains('lake', na = False) |\n",
    "           df['body'].str.contains('water', na = False) |\n",
    "           df['body'].str.contains('Ocean', na = False) |\n",
    "           df['body'].str.contains('Lake', na = False) |\n",
    "           df['body'].str.contains('Water', na = False)\n",
    "           )\n",
    "          ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test retrived data\n",
    "print(df[['subreddit.name','body']].values[11930])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subset with key words\n",
    "# later test improvement with regex\n",
    "sub_kw = df[['id','subreddit.name','body']][\n",
    "           (\n",
    "           df['body'].str.contains('micro-plastic', na = False) |\n",
    "           df['body'].str.contains('microplastic', na = False) |\n",
    "           df['body'].str.contains('Microplastic', na = False) |\n",
    "           df['body'].str.contains('Micro-plastic', na = False)\n",
    "           ) &\n",
    "           (\n",
    "           df['body'].str.contains('ocean', na = False) |\n",
    "           df['body'].str.contains('lake', na = False) |\n",
    "           df['body'].str.contains('water', na = False) |\n",
    "           df['body'].str.contains('Ocean', na = False) |\n",
    "           df['body'].str.contains('Lake', na = False) |\n",
    "           df['body'].str.contains('Water', na = False)\n",
    "           )\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check content sub-search\n",
    "sub_kw[['subreddit.name', 'body']].values[129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data cleansing functions\n",
    "# must optimize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# remove links\n",
    "def remove_links(text):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    text = re.sub(r'http\\S+', '', text) # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text) # remove bitly links\n",
    "    text = text.strip('[link]') # remove [links]\n",
    "    return text\n",
    "\n",
    "#remove HTML\n",
    "def clean_html(text):\n",
    "    html = re.sub('&lt;/?[a-z]+&gt;', '', text)\n",
    "    html = re.sub('&ft;/?[a-z]+&gt;', '', text)\n",
    "    html = re.compile('<.*?>')#regex\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "\n",
    "#remove special special characters\n",
    "def rem_spec_c(text):\n",
    "    text = re.sub('([_]+)', \"\", text)\n",
    "    return text\n",
    "\n",
    "# remove punctuation\n",
    "def clean_symb(text):\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text) # remove all except letters and spaces\n",
    "    return text\n",
    "\n",
    "# grab hashtags\n",
    "def hashtags(text):\n",
    "    hash = re.findall(r\"#(\\w+)\", text)\n",
    "    return hash\n",
    "\n",
    "# remove hashtags\n",
    "def rem_hashtags(text):\n",
    "    text = re.sub(r\"#(\\w+)\", '', text)\n",
    "    return text\n",
    "\n",
    "# remove reddit usernames\n",
    "def remove_users(text):\n",
    "    '''Takes a string and removes u/user_name'''\n",
    "    text = re.sub('(u/[A-Za-z]+[A-Za-z0-9-_]+)', '', text) \n",
    "    return text\n",
    "\n",
    "# remove twitter user\n",
    "def rem_usr_twt(text):\n",
    "    '''Takes a string and removes u/user_name'''\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text) \n",
    "    return text\n",
    "\n",
    "# translate emoji\n",
    "def emoji_conv(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        if text == None:\n",
    "            text = text\n",
    "        else:\n",
    "            text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text\n",
    "\n",
    "# remove non ascii character\n",
    "def non_ascii(s):\n",
    "    return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "# turn all in low char\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# remove emoji\n",
    "def emoji_remove(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        if text == None:\n",
    "            text = text\n",
    "        else:\n",
    "            text = text.replace(emot, ' ')\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1\n",
    "# # run polarity score on the entire dataset\n",
    "# res = {} # this is a dicitonary\n",
    "# for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#     text = row['body']\n",
    "#     myid = row['id']\n",
    "#     res[myid] = sia.polarity_scores(text)\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first 5 rows from subset\n",
    "sub_kw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleansing for rumor\n",
    "for i, row in tqdm (sub_kw.iterrows(), total=len(sub_kw)):\n",
    "    text = row['body']\n",
    "    # cleansing\n",
    "    text = lower(text)\n",
    "    text = remove_links(text)\n",
    "    text = clean_html(text)\n",
    "    text = emoji_remove(text)\n",
    "    row['body'] = text\n",
    "    myid = row['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return first row to test\n",
    "# print(sub_kw.values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run polarity score on the subset with keywords\n",
    "res = {} # this is a dictionary\n",
    "for i, row in tqdm (sub_kw.iterrows(), total=len(sub_kw)):\n",
    "    text = row['body']\n",
    "    myid = row['id']\n",
    "    res[myid] = sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store result diciotnary in a pandas dataframe the T will flip it\n",
    "vaders = pd.DataFrame(res).T\n",
    "# reset and rename index\n",
    "vaders = vaders.reset_index().rename(columns={'index': 'id'})\n",
    "vaders.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the new calcualted index to the subset previously filtered with a left merge\n",
    "vaders = vaders.merge(sub_kw, how='left') \n",
    "#vaders.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# return first row of the header column to test\n",
    "print(vaders.values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vaders.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(vaders['compound'].mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positive negative neutral association to allow group by\n",
    "conv = {}\n",
    "for i, row in tqdm (vaders.iterrows(), total=len(vaders)):\n",
    "    value = row['compound']\n",
    "    if value < 0 :\n",
    "        exp = 'negative'\n",
    "    if value > 0 :\n",
    "        exp = 'positive'\n",
    "    if value == 0 :\n",
    "        exp = 'neutral'\n",
    "    myid = row['id']\n",
    "    conv[i] = myid, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug output\n",
    "# conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn it to a dataframe\n",
    "results = pd.DataFrame(conv)\n",
    "\n",
    "# verticalized frame\n",
    "results = pd.DataFrame(conv).T\n",
    "\n",
    "# name columns\n",
    "results.columns=['id', 'type']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test rotate output with renamed columns\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attach the dataframe to the vaders results\n",
    "\n",
    "results = results.merge(vaders, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new dataframe\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "# graph results \n",
    "cmap = ListedColormap(['#e50000', '#ffff14','#0343df']) # pie\n",
    "#cmap = ListedColormap(['#0343df']) #bar\n",
    "results['type'].value_counts().sort_index().plot(\n",
    "                                            kind='pie',\n",
    "                                            title='Sentiment #Microplastic',\n",
    "                                            ylabel ='',\n",
    "                                            colormap= cmap\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDDIT WORD CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "# !pip install spacy\n",
    "# !pip install PIL\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace keyword with space\n",
    "def rem_kw (text, kw):\n",
    "    text = text.replace(kw, ' ')\n",
    "    return text\n",
    "\n",
    "test = ' &gthello&gt '\n",
    "print(rem_kw(test, '&gt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and parse\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2409706 # 1gb ram every 1000000\n",
    "words = '\\n'.join(sub_kw.body)\n",
    "words = lower(words)\n",
    "words = rem_kw(words, '&gt')\n",
    "words = remove_links(words)\n",
    "words = clean_html(words)\n",
    "words = remove_users(words)\n",
    "words = emoji_remove(words)\n",
    "words = clean_symb(words)\n",
    "words = rem_spec_c(words)\n",
    "text = nlp(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = \"\"\n",
    "for word in text:    \n",
    "    if word.pos_ in ['ADJ','NOUN','PROPN']:\n",
    "        cloud = \" \".join((cloud, word.text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      max_words=100,\n",
    "                      background_color='white',\n",
    "                      width=800,\n",
    "                      height=300).generate(cloud)\n",
    "\n",
    "plt.imshow(wordcloud,\n",
    "           interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWITTER WORD CLOUD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the csv and turn it into a dataframe\n",
    "with open ('./plastic_pollution.csv',\n",
    "'r',\n",
    "encoding=\"utf8\",\n",
    "newline='\\n') as source_csv:\n",
    "    reader = csv.reader(source_csv)\n",
    "\n",
    "    tdf = pd.read_csv(source_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "words = '\\n'.join(tdf.description)\n",
    "words = lower(words)\n",
    "words = remove_links(words)\n",
    "words = clean_html(words)\n",
    "words = rem_kw(words, '&gt')\n",
    "words = rem_hashtags(words)\n",
    "words = rem_usr_twt(words)\n",
    "words = emoji_remove(words)\n",
    "words = clean_symb(words)\n",
    "words = rem_spec_c(words)\n",
    "text = nlp(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud2 = \"\"\n",
    "for word in text:    \n",
    "    if word.pos_ in ['ADJ','NOUN','PROPN']:\n",
    "        cloud2 = \" \".join((cloud2, word.text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          max_words=100,\n",
    "                          background_color='white',\n",
    "                          width=800, height=300).generate(cloud2)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug \n",
    "# print (cloud2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
